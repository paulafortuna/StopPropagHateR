setwd("~/keras-text/R")
library(kerasR)
library(kerasR)
reticulate::use_python("/usr/bin/python")
library(kerasR)
mod <- Sequential()
ncca_res <- ncca.rsvd(X[PairedIndices,],Y[PairedIndices,], X[UnpairedIndices,],Y[UnpairedIndices,],
d = 2, hx = 0.75, hy = 0.75, nx = NumNNs_X, ny=NumNNs_Y)
library("NCCA")
N <- 10000 # Overal number of examples (train+test)
N_paired <- 5000 # Number of training examples
MaxAngle <- 4*pi
MinRadius <-0.3
MaxRadius <- 8
NumNNs_X <- 20
NumNNs_Y <- 20
sx <- 0.5
sy <- 0.5
set.seed(8409)
## Generate data for views 1,2
t <- seq(0, MaxAngle, length.out = N)
r <- seq(MinRadius, MaxRadius, length.out = N) + 2*runif(N)
#### generate X, the noise can be added!
X <- cbind(r*cos(t+0*rnorm(N)*0.05), r*sin(t+0*rnorm(N)*0.05))
X <- X + 0*matrix(rnorm(N*2), ncol = 2)
#### generate Y, the noise can be added!
Y <- cbind(t+0*rnorm(N)*1, 2*rnorm(N))
Y <- Y + 0*cbind(rep(0, N), rnorm(N))
## Training data
PairedIndices <- sample(1:N, N_paired)
## Test (or validation) data
UnpairedIndices <- setdiff(1:N,PairedIndices)
ncca_res <- ncca.rsvd(X[PairedIndices,],Y[PairedIndices,], X[UnpairedIndices,],Y[UnpairedIndices,],
d = 2, hx = 0.75, hy = 0.75, nx = NumNNs_X, ny=NumNNs_Y)
cat("The nonparametric canonical correlation between X and Y is ", ncca_res$cor_XY, "\n")
remove.packages("NCCA")
library(NCCA)
install.packages("NCCA", dependencies = TRUE)
install_github("DeveloperName/PackageName", dependencies = TRUE)
library(devtools)
install_github("DeveloperName/PackageName", dependencies = TRUE)
install_github("ilariabonavita/NCCApckg", dependencies = TRUE)
? intall_github
? install_github
library(NCCA)
N <- 10000 # Overal number of examples (train+test)
N_paired <- 5000 # Number of training examples
MaxAngle <- 4*pi
MinRadius <-0.3
MaxRadius <- 8
NumNNs_X <- 20
NumNNs_Y <- 20
sx <- 0.5
sy <- 0.5
set.seed(8409)
## Generate data for views 1,2
t <- seq(0, MaxAngle, length.out = N)
r <- seq(MinRadius, MaxRadius, length.out = N) + 2*runif(N)
#### generate X, the noise can be added!
X <- cbind(r*cos(t+0*rnorm(N)*0.05), r*sin(t+0*rnorm(N)*0.05))
X <- X + 0*matrix(rnorm(N*2), ncol = 2)
#### generate Y, the noise can be added!
Y <- cbind(t+0*rnorm(N)*1, 2*rnorm(N))
Y <- Y + 0*cbind(rep(0, N), rnorm(N))
## Training data
PairedIndices <- sample(1:N, N_paired)
## Test (or validation) data
UnpairedIndices <- setdiff(1:N,PairedIndices)
ncca_res <- ncca.rsvd(X[PairedIndices,],Y[PairedIndices,], X[UnpairedIndices,],Y[UnpairedIndices,],
d = 2, hx = 0.75, hy = 0.75, nx = NumNNs_X, ny=NumNNs_Y)
cat("The nonparametric canonical correlation between X and Y is ", ncca_res$cor_XY, "\n")
remove.packages("NCCA")
knitr::opts_chunk$set(echo = TRUE)
dataset_hs <- read.csv(file="dataset_zeerak.csv", header=TRUE, sep=",")
library(kerasR)
#library(kerasR)
test_train_preparation <- function(){
# convert to binary
dataset_hs$Hate_speech <- as.integer(dataset_hs$Class != "neither")
dataset_hs$Sexism <- as.integer(dataset_hs$Class == "sexism")
dataset_hs$Racism <- as.integer(dataset_hs$Class == "racism")
# Split the data into a training set and a validation set
# But assure shuffled data
training_samples <- as.integer(0.7*nrow(dataset_hs))      # We will be training on 200 samples # divide in 70% and 30%
validation_samples <- nrow(dataset_hs) - training_samples   # We will be validating on 10000 samples
indices <- 1:nrow(dataset_hs)
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1):
(training_samples + validation_samples)]
# convert to binary
labels_hate_speech <- as.integer(dataset_hs$Hate_speech)
labels_sexism <- as.integer(dataset_hs$Sexism)
labels_racism <- as.integer(dataset_hs$Racism)
labels_hate_speech <- as.array(labels_hate_speech)
labels_sexism <- as.array(labels_sexism)
labels_racism <- as.array(labels_racism)
x_train <<- dataset_hs[training_indices,]
y_train_hate_speech <<- labels_hate_speech[training_indices]
y_train_sexism <<- labels_sexism[training_indices]
y_train_racism <<- labels_racism[training_indices]
x_val <<- dataset_hs[validation_indices,]
y_val_hate_speech <<- labels_hate_speech[validation_indices]
y_val_sexism <<- labels_sexism[validation_indices]
y_val_racism <<- labels_racism[validation_indices]
}
test_train_preparation()
remove(dataset_hs)
# Remove RT and links.
data_pre_processing <- function(dataset){
# Get rid of URLs
dataset$text <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", dataset$text)
# Take out retweet header, there is only one
dataset$text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", dataset$text)
return(dataset)
}
x_train <- data_pre_processing(x_train)
# from https://github.com/kylehamilton/deep-learning-with-r-notebooks/blob/master/notebooks/6.1-using-word-embeddings.Rmd
#reticulate::use_python("/Users/paulafortuna/anaconda3/envs/r-tensorflow/bin/python")
maxlen <- 100                 # We will cut reviews after 100 words
max_words <- 10000            # We will only consider the top 10,000 words in the dataset
library(keras)
install.packages("keras")
knitr::opts_chunk$set(echo = TRUE)
dataset_hs <- read.csv(file="dataset_zeerak.csv", header=TRUE, sep=",")
#library(kerasR)
test_train_preparation <- function(){
# convert to binary
dataset_hs$Hate_speech <- as.integer(dataset_hs$Class != "neither")
dataset_hs$Sexism <- as.integer(dataset_hs$Class == "sexism")
dataset_hs$Racism <- as.integer(dataset_hs$Class == "racism")
# Split the data into a training set and a validation set
# But assure shuffled data
training_samples <- as.integer(0.7*nrow(dataset_hs))      # We will be training on 200 samples # divide in 70% and 30%
validation_samples <- nrow(dataset_hs) - training_samples   # We will be validating on 10000 samples
indices <- 1:nrow(dataset_hs)
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1):
(training_samples + validation_samples)]
# convert to binary
labels_hate_speech <- as.integer(dataset_hs$Hate_speech)
labels_sexism <- as.integer(dataset_hs$Sexism)
labels_racism <- as.integer(dataset_hs$Racism)
labels_hate_speech <- as.array(labels_hate_speech)
labels_sexism <- as.array(labels_sexism)
labels_racism <- as.array(labels_racism)
x_train <<- dataset_hs[training_indices,]
y_train_hate_speech <<- labels_hate_speech[training_indices]
y_train_sexism <<- labels_sexism[training_indices]
y_train_racism <<- labels_racism[training_indices]
x_val <<- dataset_hs[validation_indices,]
y_val_hate_speech <<- labels_hate_speech[validation_indices]
y_val_sexism <<- labels_sexism[validation_indices]
y_val_racism <<- labels_racism[validation_indices]
}
test_train_preparation()
remove(dataset_hs)
# Remove RT and links.
data_pre_processing <- function(dataset){
# Get rid of URLs
dataset$text <- gsub(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", dataset$text)
# Take out retweet header, there is only one
dataset$text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", dataset$text)
return(dataset)
}
x_train <- data_pre_processing(x_train)
# from https://github.com/kylehamilton/deep-learning-with-r-notebooks/blob/master/notebooks/6.1-using-word-embeddings.Rmd
#reticulate::use_python("/Users/paulafortuna/anaconda3/envs/r-tensorflow/bin/python")
maxlen <- 100                 # We will cut reviews after 100 words
max_words <- 10000            # We will only consider the top 10,000 words in the dataset
library(keras)
tokenization <- function(maxlen, max_words, dataset, file_sufix){
tokenizer <- keras::text_tokenizer(num_words = max_words) %>%
keras::fit_text_tokenizer(dataset$text)
#keras::save_text_tokenizer(tokenizer, paste0("tokenizer", file_sufix))
sequences <- keras::texts_to_sequences(tokenizer, dataset$text)
word_index <<- tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
data <- keras::pad_sequences(sequences, maxlen = maxlen)
return(data)
}
x_train <- tokenization(maxlen, max_words, x_train, "_en")
lstm_model <- function(x_train, y_train, file_prefix){
# extracted from https://cran.r-project.org/web/packages/kerasR/vignettes/introduction.html
X_train <- keras::pad_sequences(x_train, maxlen = 100)
Y_train <- y_train
library(kerasR)
# including a Long-Short Term Memory Unit (LSTM)
mod <- Sequential()
mod$add(Embedding(10000, 32, input_length = 100, input_shape = c(100)))
mod$add(Dropout(0.25))
mod$add(LSTM(32))
mod$add(Dense(256))
mod$add(Dropout(0.25))
mod$add(Activation('relu'))
mod$add(Dense(1))
mod$add(Activation('sigmoid'))
keras_compile(mod,  loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.00025))
keras_fit(mod, X_train, Y_train, batch_size = 32, epochs = 10, verbose = 1,
validation_split = 0.1)
filename <- paste0("models/", file_prefix)
filename <- paste0(filename, ".h5")
keras_save(mod, path = filename)
}
lstm_model(x_train, y_train_hate_speech, "hate_en")
install.packages("kerasR")
library("devtools")
setwd("~/StopPropagHateR.git/StopPropagHateR")
setwd("~/StopPropagHateR.git")
install("StopPropagHateR")
library(StopPropagHateR)
? stopPropagHate
texts_data_frame <- data.frame(id = c(1,2,3), text = c("Lugar de mulher e na cozinha, isto e a verdade", "gorda e feia", "mais uma mensagem de teste"))
stopPropagHate(texts_data_frame, "sexism", "pt")
print(getwd())
setwd("~/StopPropagHateR.git")
library("devtools")
install("StopPropagHateR")
library(StopPropagHateR)
? stopPropagHate
texts_data_frame <- data.frame(id = c(1,2,3), text = c("Lugar de mulher e na cozinha, isto e a verdade", "gorda e feia", "mais uma mensagem de teste"))
stopPropagHate(texts_data_frame, "sexism", "pt")
remove.packages(StopPropagHateR)
remove.packages("StopPropagHateR")
